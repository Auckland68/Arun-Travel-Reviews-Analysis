{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb5ad7b",
   "metadata": {},
   "source": [
    "# Notebook14 Combined Aspect Sentiment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2db5c",
   "metadata": {},
   "source": [
    "# Section 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "fc48f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Embedding,Dropout\n",
    "import contractions\n",
    "import pickle\n",
    "import string\n",
    "import ast\n",
    "import os\n",
    "from spellchecker import SpellChecker\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e384041",
   "metadata": {},
   "source": [
    "# Section 2: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "01abf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_accom = pd.read_csv(\"X_train_accom.csv\",index_col = 0, squeeze = True)\n",
    "X_val_accom = pd.read_csv(\"X_val_accom.csv\",index_col = 0, squeeze = True)\n",
    "X_test_accom = pd.read_csv(\"X_test_accom.csv\",index_col = 0, squeeze = True)\n",
    "\n",
    "X_train_food = pd.read_csv(\"X_train_food.csv\",index_col = 0, squeeze = True)\n",
    "X_val_food = pd.read_csv(\"X_val_food.csv\",index_col = 0, squeeze = True)\n",
    "X_test_food = pd.read_csv(\"X_test_food.csv\",index_col = 0, squeeze = True)\n",
    "\n",
    "X_train_attract = pd.read_csv(\"X_train_attract.csv\",index_col = 0, squeeze = True)\n",
    "X_val_attract = pd.read_csv(\"X_val_attract.csv\",index_col = 0, squeeze = True)\n",
    "X_test_attract = pd.read_csv(\"X_test_attract.csv\",index_col = 0, squeeze = True)\n",
    "\n",
    "y_train_accom = pd.read_csv(\"y_train_accom.csv\",index_col = 0, squeeze = True)\n",
    "y_val_accom = pd.read_csv(\"y_val_accom.csv\",index_col = 0, squeeze = True)\n",
    "y_test_accom = pd.read_csv(\"y_test_accom.csv\",index_col = 0, squeeze = True)\n",
    "\n",
    "y_train_food = pd.read_csv(\"y_train_food.csv\",index_col = 0, squeeze = True)\n",
    "y_val_food = pd.read_csv(\"y_val_food.csv\",index_col = 0, squeeze = True)\n",
    "y_test_food = pd.read_csv(\"y_test_food.csv\",index_col = 0, squeeze = True)\n",
    "\n",
    "y_train_attract = pd.read_csv(\"y_train_attract.csv\",index_col = 0, squeeze = True)\n",
    "y_val_attract = pd.read_csv(\"y_val_attract.csv\",index_col = 0, squeeze = True)\n",
    "y_test_attract = pd.read_csv(\"y_test_attract.csv\",index_col = 0, squeeze = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcd921",
   "metadata": {},
   "source": [
    "# Section 3: Functions to encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "0138f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab(y_train, y_val, y_test):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y_train_l = le.fit_transform(y_train)\n",
    "    y_val_l = le.fit_transform(y_val)\n",
    "    y_test_l = le.transform(y_test)\n",
    "    return y_train_l,y_val_l, y_test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "id": "314be02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin(y_train, y_val, y_test):\n",
    "    binarizer = preprocessing.LabelBinarizer()\n",
    "    y_train_e = binarizer.fit_transform(y_train)\n",
    "    y_val_e = binarizer.transform(y_val)\n",
    "    y_test_e = binarizer.transform(y_test)\n",
    "    return y_train_e,y_val_e,y_test_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "id": "cdb029d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok(X_train, X_val, X_test):\n",
    "    tokenizer = Tokenizer(num_words = 6000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train_s = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_s = tokenizer.texts_to_sequences(X_val)\n",
    "    X_test_s = tokenizer.texts_to_sequences(X_test)\n",
    "    X_train_w = pad_sequences(np.array(X_train_s,dtype = \"object\"), maxlen=15, padding=\"post\", truncating=\"post\", value=0.0)\n",
    "    X_val_w = pad_sequences(np.array(X_val_s,dtype = \"object\"), maxlen=15, padding=\"post\", truncating=\"post\", value=0.0)\n",
    "    X_test_w = pad_sequences(np.array(X_test_s, dtype = \"object\"), maxlen = 15, padding = \"post\", truncating = \"post\", value = 0.0)\n",
    "    \n",
    "    return X_train_w, X_val_w, X_test_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834cac4",
   "metadata": {},
   "source": [
    "### Apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "5f9c0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function 1 and 2\n",
    "y_train_accom_l, y_val_accom_l, y_test_accom_l = lab(y_train_accom, y_val_accom, y_test_accom)\n",
    "y_train_food_l, y_val_food_l, y_test_food_l = lab(y_train_food, y_val_food, y_test_food)\n",
    "y_train_attract_l, y_val_attract_l, y_test_attract_l = lab(y_train_attract, y_val_attract, y_test_attract)\n",
    "\n",
    "y_train_accom_e, y_val_accom_e, y_test_accom_e = bin(y_train_accom, y_val_accom, y_test_accom)\n",
    "y_train_food_e, y_val_food_e, y_test_food_e = bin(y_train_food, y_val_food, y_test_food)\n",
    "y_train_attract_e, y_val_attract_e, y_test_attract_e = bin(y_train_attract, y_val_attract, y_test_attract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "c671497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function 3\n",
    "X_train_accom_w, X_val_accom_w, X_test_accom_w = tok(X_train_accom, X_val_accom, X_test_accom)\n",
    "X_train_food_w, X_val_food_w, X_test_food_w = tok(X_train_food, X_val_food, X_test_food)\n",
    "X_train_attract_w, X_val_attract_w, X_test_attract_w = tok(X_train_attract, X_val_attract, X_test_attract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "107918a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1776, 15) (1776, 7)\n",
      "(1680, 15) (1680, 6)\n",
      "(1320, 15) (1320, 7)\n"
     ]
    }
   ],
   "source": [
    "# Get shape of training, validation and test sets\n",
    "print(X_train_accom_w.shape,y_train_accom_e.shape)\n",
    "print(X_train_food_w.shape, y_train_food_e.shape)\n",
    "print(X_train_attract_w.shape, y_train_attract_e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6eca08",
   "metadata": {},
   "source": [
    "# Section 4: Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "0e3235d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apect model 1 for use with accommodation and attractions\n",
    "modela = Sequential()\n",
    "modela.add(Embedding(input_dim = 6000, output_dim = 16, input_length = 15))\n",
    "modela.add(Flatten())\n",
    "modela.add(Dense(512, activation='relu', input_shape=(1600,)))\n",
    "modela.add(Dropout(rate=0.5))\n",
    "modela.add(Dense(256,activation = 'relu'))\n",
    "modela.add(Dropout(rate=0.5))\n",
    "modela.add(Dense(128,activation = 'relu'))\n",
    "modela.add(Dropout(rate=0.5))\n",
    "modela.add(Dense(7, activation='softmax'))\n",
    "modela.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#modela.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "87bef753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect model 2 for use with food\n",
    "modelb = Sequential()\n",
    "modelb.add(Embedding(input_dim = 6000, output_dim = 16, input_length = 15))\n",
    "modelb.add(Flatten())\n",
    "modelb.add(Dense(512, activation='relu', input_shape=(1600,)))\n",
    "modelb.add(Dropout(rate=0.5))\n",
    "modelb.add(Dense(256,activation = 'relu'))\n",
    "modelb.add(Dropout(rate=0.5))\n",
    "modelb.add(Dense(128,activation = 'relu'))\n",
    "modelb.add(Dropout(rate=0.5))\n",
    "modelb.add(Dense(5, activation='softmax'))\n",
    "modelb.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])     \n",
    "#modelb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "f70a61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment model 1 for use for all categories\n",
    "modelc = Sequential()\n",
    "modelc.add(Embedding(input_dim = 6000, output_dim = 16, input_length=20))\n",
    "modelc.add(Flatten())\n",
    "modelc.add(Dense(64, activation='relu', input_shape=(1600,)))\n",
    "modelc.add(Dense(1, activation='sigmoid'))\n",
    "modelc.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#modelc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "62643d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run weights function to get weights\n",
    "def weight(y_train):\n",
    "    class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "e171404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory and callbacks\n",
    "def out(name):\n",
    "    output_dir = name\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    return output_dir\n",
    "        \n",
    "def call(output_dir,patience):\n",
    "    modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "    callbacks = [EarlyStopping(monitor = \"val_loss\", patience = patience),modelcheckpoint]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "d14e5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mod(model, X_train, X_val, y_train, y_val):\n",
    "    val_acc = []\n",
    "    history = model.fit(X_train, y_train,\n",
    "                       epochs = 50,\n",
    "                       verbose = 2,\n",
    "                       callbacks = callbacks,\n",
    "                       validation_data = (X_val, y_val),\n",
    "                       batch_size = 10,\n",
    "                       class_weight = class_weights)\n",
    "    print(\"\\n\\n\")\n",
    "    print(model.evaluate(X_train, y_train))\n",
    "    print(model.evaluate(X_val, y_val))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9020b2e",
   "metadata": {},
   "source": [
    "### Apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "39884c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'Aspect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-924-fcff7640541a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Class weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcw_accom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_accom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcw_food\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_food\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcw_attract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_attract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Neural\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5463\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5464\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5465\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'Aspect'"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "cw_accom = weight(y_train_accom.Aspect)\n",
    "cw_food = weight(y_train_food.Aspect)\n",
    "cw_attract = weight(y_train_attract.Aspect)\n",
    "\n",
    "class_weights_accom = {0:cw_accom[0],1:cw_accom[1],2:cw_accom[2],3:cw_accom[3],4:cw_accom[4],5:cw_accom[5],6:cw_accom[6]}\n",
    "class_weights_food = {0:cw_food[0],1:cw_food[1],2:cw_food[2],3:cw_food[3],4:cw_food[4],5:cw_food[5]}\n",
    "class_weights_attract = {0:cw_attract[0],1:cw_attract[1],2:cw_attract[2],3:cw_attract[3],4:cw_attract[4],5:cw_attract[5],6:cw_attract[6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "d6c9694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "178/178 - 0s - loss: 1.7560 - accuracy: 0.2584 - val_loss: 1.1431 - val_accuracy: 0.6036\n",
      "Epoch 2/50\n",
      "178/178 - 0s - loss: 0.7686 - accuracy: 0.7280 - val_loss: 0.4054 - val_accuracy: 0.9099\n",
      "Epoch 3/50\n",
      "178/178 - 0s - loss: 0.2972 - accuracy: 0.9240 - val_loss: 0.3041 - val_accuracy: 0.9234\n",
      "Epoch 4/50\n",
      "178/178 - 0s - loss: 0.1464 - accuracy: 0.9657 - val_loss: 0.2826 - val_accuracy: 0.9369\n",
      "Epoch 5/50\n",
      "178/178 - 0s - loss: 0.0758 - accuracy: 0.9859 - val_loss: 0.2994 - val_accuracy: 0.9324\n",
      "Epoch 6/50\n",
      "178/178 - 0s - loss: 0.0579 - accuracy: 0.9842 - val_loss: 0.2662 - val_accuracy: 0.9324\n",
      "Epoch 7/50\n",
      "178/178 - 0s - loss: 0.0438 - accuracy: 0.9910 - val_loss: 0.2938 - val_accuracy: 0.9369\n",
      "Epoch 8/50\n",
      "178/178 - 0s - loss: 0.0388 - accuracy: 0.9916 - val_loss: 0.2585 - val_accuracy: 0.9414\n",
      "Epoch 9/50\n",
      "178/178 - 0s - loss: 0.0224 - accuracy: 0.9944 - val_loss: 0.3115 - val_accuracy: 0.9369\n",
      "Epoch 10/50\n",
      "178/178 - 0s - loss: 0.0348 - accuracy: 0.9927 - val_loss: 0.4563 - val_accuracy: 0.9234\n",
      "Epoch 11/50\n",
      "178/178 - 0s - loss: 0.0237 - accuracy: 0.9938 - val_loss: 0.3059 - val_accuracy: 0.9369\n",
      "Epoch 12/50\n",
      "178/178 - 0s - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.3771 - val_accuracy: 0.9279\n",
      "Epoch 13/50\n",
      "178/178 - 0s - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.3913 - val_accuracy: 0.9234\n",
      "\n",
      "\n",
      "\n",
      "56/56 [==============================] - 0s 427us/step - loss: 0.0100 - accuracy: 0.9972\n",
      "[0.010031850077211857, 0.997184693813324]\n",
      "7/7 [==============================] - 0s 570us/step - loss: 0.3913 - accuracy: 0.9234\n",
      "[0.39126336574554443, 0.9234234094619751]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run model 1 - Accommodation\n",
    "output_dir = out('model_output/Accommodation')\n",
    "callbacks = call(output_dir,5)\n",
    "class_weights = class_weights_accom\n",
    "\n",
    "history1 = run_mod(modela, X_train_accom_w, X_val_accom_w, y_train_accom_e, y_val_accom_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e07f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model 2 - Food\n",
    "output_dir = out('model_output/Food')\n",
    "callbacks = call(output_dir,5)\n",
    "class_weights = class_weights_food\n",
    "\n",
    "history1 = run_mod(modelb, X_train_food_w, X_val_food_w, y_train_food_e, y_val_food_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f7566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00548bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c54f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e96d46",
   "metadata": {},
   "source": [
    "## Function 1 - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd446d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(review)\n",
    "    text_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "7e269a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean sentences\n",
    "\n",
    "def process(text):\n",
    "    if text != []:\n",
    "        text = text.replace('\\n',' ')\n",
    "        text = text.strip().lower()\n",
    "        text = text.replace('xmas','christmas')\n",
    "        text = text.replace('\\£',\"\") \n",
    "        text = text.replace(r'\\/',\" \") \n",
    "        text = text.replace('\\d+\\-\\d+',\"\") \n",
    "        text = text.replace('\\d+\\w{2}',\"\") \n",
    "        text = text.replace('\\.{3,}',\"\") \n",
    "        text = text.replace(' i ',\"\")\n",
    "        text = text.replace(' le ',\"\")\n",
    "        text = contractions.fix(text)\n",
    "        text = nltk.word_tokenize(text)\n",
    "        punc = string.punctuation\n",
    "        text = [word for word in text if word not in punc]\n",
    "        text = [n for n in text if not n.isnumeric()]\n",
    "        text = [e for e in text if e.encode(\"ascii\",\"ignore\")]\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop_remove = [\"not\",\"don't\",\"didn't\",\"wasn't\",\"won't\",\"isn't\"]\n",
    "        stop1 = [w for w in stop if w not in stop_remove]\n",
    "        add_stop = ['etc','read','read less','lot','butlins', 'bognor','regis','b',' i ','..','arundel castle','premier','inn','u',\n",
    "                    'castle',\"year\",\"hilton\",\"time\",\"day\",\"shoreline\",\"oyster\",\"bay\",\"church farm\",\"hotham\",\"hotham park\",\n",
    "                    \"hawk walk\",\"hawk\",\"arundel\",\"littlehampton\"]\n",
    "        stop1.extend(add_stop)\n",
    "        text = [w for w in text if w not in stop1]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = [lemmatizer.lemmatize(w) for w in text ]\n",
    "        spell = SpellChecker()\n",
    "        word_list = []\n",
    "        for w in text:\n",
    "            new = spell.correction(w)\n",
    "            if new != w:\n",
    "                word_list.append(new)\n",
    "            else:\n",
    "                word_list.append(w)\n",
    "            text_joined = ' '.join(word_list) \n",
    "            \n",
    "                       \n",
    "    return text_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "f91e7924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonderful place stay'"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text = process(\"This was a wonderful place to stay\")\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07981d24",
   "metadata": {},
   "source": [
    "## Function 2 - Aspect Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "ab8ddc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text):\n",
    "    text = word_tokenize(text)\n",
    "    text_pos = nltk.tag.pos_tag(text)\n",
    "    noun = [i[0] for i in text_pos if i[1].startswith('N')]\n",
    "    return noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "8e9f0cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place', 'stay']"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254afc8",
   "metadata": {},
   "source": [
    "## Function 3 - Noun Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "dc7c8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_extract(text):\n",
    "    blob = TextBlob(text)\n",
    "    noun_phrases = blob.noun_phrases\n",
    "    noun_phrases = ' '.join(noun_phrases)\n",
    "    return noun_phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "d6f52f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonderful place stay'"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_extract(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f4872",
   "metadata": {},
   "source": [
    "## Function 4 - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b95dd",
   "metadata": {},
   "source": [
    "## Function 4 - Encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fafed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text,category):\n",
    "    if category == \"accommodation\":\n",
    "        aspects_accom = tokenizer.fit_on_texts(X_train)\n",
    "        aspects_s = tokenizer.texts_to_sequences(nouns)\n",
    "        aspects_w = pad_sequences(np.array(aspects_s, dtype = \"object\"), maxlen = 15,padding = \"post\", truncating = \"post\", value = 0.0)\n",
    "        model = load_model('accommodation.h5') \n",
    "                \n",
    "    elif category == \"food\":\n",
    "        with open('tok_food.pickle',rb) as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "            aspects_s = tokenizer.texts_to_sequences(nouns)   \n",
    "            aspects_w = pad_sequences(np.array(aspects_s, dtype = \"object\"), maxlen = 15, padding = \"post\", truncating = \"post\", value = 0.0)\n",
    "    \n",
    "    else:\n",
    "        with open('tok_attract.pickle',rb) as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "            aspects_s = tokenizer.texts_to_sequences(nouns)  \n",
    "            aspects_w = pad_sequences(np.array(aspects_s, dtype = \"object\"), maxlen = 15, padding = \"post\", truncating = \"post\", value = 0.0)  \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3436f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f634e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ebf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e122d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4265832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "1c4494b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-521-976a8a52f78f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-521-976a8a52f78f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    if category == \"accommodation\":\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Noun encoding\n",
    "        \n",
    "                        \n",
    "        \n",
    "            \n",
    "        \n",
    "                             \n",
    "        # Aspect prediction\n",
    "        \n",
    "                \n",
    "        # Sentiment prediction\n",
    "        with open('tok_sent.pickle', 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "        sent_s = tokenizer.texts_to_sequences(phrases)\n",
    "        sent_w = pad_sequences(np.array(sent_s,dtype = \"object\"), maxlen = 15, padding = \"post\",truncating = \"post\",value = 0.0)\n",
    "        model_s = load_model('Sentiment.h5') \n",
    "        sent_pred = model_s.predict(sent_w)\n",
    "        sent_class = (model_s.predict(X_val_w) > 0.5).astype(\"int32\")\n",
    "        \n",
    "        return  sent_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213ce22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "a966a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pre-process text and run through aspect and sentiment model. Input is expected to be in a text format\n",
    "# e.g. \"I love this place\"\n",
    "\n",
    "def full_model(text):\n",
    "                   \n",
    "        # Preprocessing text\n",
    "        text = text.replace('\\n',' ')\n",
    "        text = text.strip().lower()\n",
    "        text = text.replace('xmas','christmas')\n",
    "        text = text.replace('\\£',\"\") \n",
    "        text = text.replace(r'\\/',\" \") \n",
    "        text = text.replace('\\d+\\-\\d+',\"\") \n",
    "        text = text.replace('\\d+\\w{2}',\"\") \n",
    "        text = text.replace('\\.{3,}',\"\") \n",
    "        text = text.replace(' i ',\"\")\n",
    "        text = text.replace(' le ',\"\")\n",
    "        text = contractions.fix(text)\n",
    "        text = nltk.word_tokenize(text)\n",
    "        punc = string.punctuation\n",
    "        text = [word for word in text if word not in punc]\n",
    "        text = [n for n in text if not n.isnumeric()]\n",
    "        text = [e for e in text if e.encode(\"ascii\",\"ignore\")]\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop_remove = [\"not\",\"don't\",\"didn't\",\"wasn't\",\"won't\",\"isn't\"]\n",
    "        stop1 = [w for w in stop if w not in stop_remove]\n",
    "        add_stop = ['etc','read','read less','lot','butlins', 'bognor','regis','b',' i ','..','arundel castle','premier','inn','u',\n",
    "                    'castle',\"year\",\"hilton\",\"time\",\"day\",\"shoreline\",\"oyster\",\"bay\",\"church farm\",\"hotham\",\"hotham park\",\n",
    "                    \"hawk walk\",\"hawk\",\"arundel\",\"littlehampton\"]\n",
    "        stop1.extend(add_stop)\n",
    "        text = [w for w in text if w not in stop1]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = [lemmatizer.lemmatize(w) for w in text ]\n",
    "        spell = SpellChecker()\n",
    "        word_list = []\n",
    "        for w in text:\n",
    "            new = spell.correction(w)\n",
    "            if new != w:\n",
    "                word_list.append(new)\n",
    "            else:\n",
    "                word_list.append(w)\n",
    "        text_joined = ' '.join(word_list)\n",
    "        \n",
    "        # Noun extraction\n",
    "        text_pos = nltk.tag.pos_tag(word_list)\n",
    "        nouns = [i[0] for i in text_pos if i[1].startswith('N')]\n",
    "                \n",
    "        # Noun phrase extraction\n",
    "        blob = TextBlob(text_joined)\n",
    "        noun_phrases = blob.noun_phrases\n",
    "        phrase_list = []\n",
    "        for item in noun_phrases:\n",
    "            phrase_list.append(item)\n",
    "                            \n",
    "        return nouns, phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "f6768a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run reviews - enter review text and category of review\n",
    "\n",
    "def review_analyser(review):\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cceefb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
