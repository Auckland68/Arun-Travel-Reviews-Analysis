{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the data is pre-processed:\n",
    "\n",
    "* Section 1 - pre-processing\n",
    "* Section 2 - text cleaning processes demonstrated on a toy dataset\n",
    "* Section 3 - text cleaning against dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install language_check - note pyahocorasick had to also be installed using --add channels conda-forge and\n",
    "# conda install pyahocorasick. Java also installed in the path.\n",
    "# ! pip install --upgrade language-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install contractions\n",
    "#! pip install pyspellchecker \n",
    "#! pip install autocorrect\n",
    "#!pip install Gensim\n",
    "#! conda update pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\imoge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\imoge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\imoge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize,sent_tokenize\n",
    "import string\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in balanced datasets from Post Attributes Base notebook\n",
    "train = pd.read_csv(\"combined_train.csv\",index_col = 0)\n",
    "val = pd.read_csv(\"combined_val.csv\",index_col = 0)\n",
    "test = pd.read_csv(\"combined_test.csv\",index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Town</th>\n",
       "      <th>Type</th>\n",
       "      <th>Contributions</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Date</th>\n",
       "      <th>LocCode</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Trevali Buest House</td>\n",
       "      <td>Accommodation</td>\n",
       "      <td>Bognor</td>\n",
       "      <td>B&amp;B/Inn</td>\n",
       "      <td>315</td>\n",
       "      <td>Central B &amp; B</td>\n",
       "      <td>We had room 6, excellent view, we could see th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Sea View</td>\n",
       "      <td>Accommodation</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>50</td>\n",
       "      <td>Not what it used to be...</td>\n",
       "      <td>We lived in the area for 25 years and in fact,...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name       Category           Town     Type  \\\n",
       "245  Trevali Buest House  Accommodation         Bognor  B&B/Inn   \n",
       "209             Sea View  Accommodation  Littlehampton    Hotel   \n",
       "\n",
       "     Contributions                      Title  \\\n",
       "245            315              Central B & B   \n",
       "209             50  Not what it used to be...   \n",
       "\n",
       "                                                Review  Date  LocCode Cuisine  \\\n",
       "245  We had room 6, excellent view, we could see th...   1.0      2.0       0   \n",
       "209  We lived in the area for 25 years and in fact,...   2.0      2.0       0   \n",
       "\n",
       "     Score  \n",
       "245      0  \n",
       "209      1  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1453, 11)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Preprocessing the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing tasks:\n",
    "* join title and review text together into one column\n",
    "* set bad ratings of 1&2 to 1 and good of 4&5 to 0 and drop those rated 3\n",
    "* split reviews in dataframes for each category\n",
    "* sample 'good' reviews to match number of 'bad' to create balanced datasets\n",
    "* split into train, validation and testing sets for each category stratifying the y values so the same proportions appear\n",
    "  in each of train,val and test sets.\n",
    "* recombine the category feature dataframes to create combined feature dataframes for training, validation and test\n",
    "* concat the dataframes to create 3 balanced final dataframes with features and rating for accom, food and attractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select columns of interest, join title and review, drop unwanted columns and reset index\n",
    "def proc(df):\n",
    "    df = df[[\"Town\",\"Category\",\"Title\",\"Review\",\"Score\"]]\n",
    "    df[\"all_text\"] = df[\"Title\"] +\" \"+ df[\"Review\"]\n",
    "    df.drop(columns = [\"Title\",\"Review\"],axis = 1, inplace = True)\n",
    "    df.reset_index(inplace = True)\n",
    "    df.columns = [\"OrgInd\",\"Town\",\"Category\",\"Score\",\"all_text\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = proc(train)\n",
    "df_val = proc(val)\n",
    "df_test = proc(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrgInd</th>\n",
       "      <th>Town</th>\n",
       "      <th>Category</th>\n",
       "      <th>Score</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245</td>\n",
       "      <td>Bognor</td>\n",
       "      <td>Accommodation</td>\n",
       "      <td>0</td>\n",
       "      <td>Central B &amp; B We had room 6, excellent view, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Accommodation</td>\n",
       "      <td>1</td>\n",
       "      <td>Not what it used to be... We lived in the area...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrgInd           Town       Category  Score  \\\n",
       "0     245         Bognor  Accommodation      0   \n",
       "1     209  Littlehampton  Accommodation      1   \n",
       "\n",
       "                                            all_text  \n",
       "0  Central B & B We had room 6, excellent view, w...  \n",
       "1  Not what it used to be... We lived in the area...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Text Cleaning against a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrgInd</th>\n",
       "      <th>Town</th>\n",
       "      <th>Category</th>\n",
       "      <th>Score</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>564</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Food</td>\n",
       "      <td>1</td>\n",
       "      <td>Best avoided As we were told the caf√© was full...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>245</td>\n",
       "      <td>Bognor</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "      <td>First class food in tasteful surroundings I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>299</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Food</td>\n",
       "      <td>1</td>\n",
       "      <td>Lunchtime chaos Booked table for 4 several wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>216</td>\n",
       "      <td>Bognor</td>\n",
       "      <td>Accommodation</td>\n",
       "      <td>1</td>\n",
       "      <td>Excessive cost Premier Inn charged me. I was c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>138</td>\n",
       "      <td>Arundel</td>\n",
       "      <td>Accommodation</td>\n",
       "      <td>0</td>\n",
       "      <td>Charming hotel I booked a stay with dinner, sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>157</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "      <td>What a secret gem What a beautiful treat. We w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>704</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "      <td>Family run restaurant with amazing food Amazin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>992</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Food</td>\n",
       "      <td>1</td>\n",
       "      <td>Disappointing Where do i start\\n\\nThe so calle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>1168</td>\n",
       "      <td>Arundel</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "      <td>super brunch in lovely surroundings Popped in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>998</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Food</td>\n",
       "      <td>1</td>\n",
       "      <td>Probably the unfriendliest place I have ever v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      OrgInd           Town       Category  Score  \\\n",
       "1352     564  Littlehampton           Food      1   \n",
       "482      245         Bognor           Food      0   \n",
       "1309     299  Littlehampton           Food      1   \n",
       "270      216         Bognor  Accommodation      1   \n",
       "278      138        Arundel  Accommodation      0   \n",
       "665      157  Littlehampton           Food      0   \n",
       "1012     704  Littlehampton           Food      0   \n",
       "1166     992  Littlehampton           Food      1   \n",
       "1099    1168        Arundel           Food      0   \n",
       "1062     998  Littlehampton           Food      1   \n",
       "\n",
       "                                               all_text  \n",
       "1352  Best avoided As we were told the caf√© was full...  \n",
       "482   First class food in tasteful surroundings I ha...  \n",
       "1309  Lunchtime chaos Booked table for 4 several wee...  \n",
       "270   Excessive cost Premier Inn charged me. I was c...  \n",
       "278   Charming hotel I booked a stay with dinner, sp...  \n",
       "665   What a secret gem What a beautiful treat. We w...  \n",
       "1012  Family run restaurant with amazing food Amazin...  \n",
       "1166  Disappointing Where do i start\\n\\nThe so calle...  \n",
       "1099  super brunch in lovely surroundings Popped in ...  \n",
       "1062  Probably the unfriendliest place I have ever v...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up test dataframe (uncomment to run a new sample)\n",
    "test = df_train.sample(10,random_state = 0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace and set to lowercase\n",
    "test['lower'] = test[\"all_text\"].apply(lambda x: x.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first class food in tasteful surroundings i have lunched twice at mustards within the last few days.\\n\\nmy first lunch included last sunday was a delicious homemade broccoli soup and a generous, equally delicious, portion of roast lamb, yorkshire pudding, roast potatoes, gravy and two side dishes with freshly cooked vegetables. in addition there was warm, home baked bread and a carafe of water.\\n\\ntoday being less hungry i chose fried cod, chips and mashed peas; all cooked to perfection. once again there was freshly baked bread. afterwards i ate homemade lemon, lime and raspberry sorbets - so much better than th√© mass produced variety.\\n\\non both occasions the service was efficient and friendly. mustards is stylishly furnished and attention has been paid to many details: for example: cloth serviettes and cloth towels in the immaculately clean bathroom.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample review\n",
    "test[\"lower\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words, and remove the 'read more', 'read less' tags (not relevant to example review)\n",
    "test['clean'] = test[\"lower\"].replace({'xmas': 'christmas'}, regex=True)\n",
    "test['clean'] = test.clean.str.replace(r'\\read less$', '', regex=True).str.strip()\n",
    "test['clean'] = test.clean.str.replace(r'\\read more$', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove other characters, split two words separated with slash, remove digits plus am and pm, \n",
    "# remove digits plus th or nd indicating dates, multiple full stops not removed by punctuation\n",
    "\n",
    "test['clean'] = test[\"clean\"].replace({'\\¬£':''}, regex = True)\n",
    "test['clean'] = test[\"clean\"].replace(r'\\/',\" \", regex=True)\n",
    "test['clean'] = test[\"clean\"].replace({'\\d+\\-\\d+':\"\"}, regex = True)\n",
    "test['clean'] = test[\"clean\"].replace({'\\d+\\w{2}':\"\"}, regex = True)\n",
    "test['clean'] = test[\"clean\"].replace({'\\.{3,}':\"\"}, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"best avoided as we were told the caf√© was full (even though there were empty tables) we foolishly went to the kiosk outside to order food and drinks. it was barely organised chaos. the queue just didn't move, while they faffed about pretending to know how to prepare food and drinks. when we finally got to order they had run out of most things we wanted on the limited menu (sausage rolls and sandwiches) so we ended up waiting 20 mins for some chips to be cooked, which was only a small portion. the tables out on the beach were dirty, with rubbish underneath them. a poorly managed place, which clearly trades on its location.\""
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "test[\"clean\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "test[\"contracts\"] = test[\"clean\"].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best avoided as we were told the caf√© was full (even though there were empty tables) we foolishly went to the kiosk outside to order food and drinks. it was barely organised chaos. the queue just did not move, while they faffed about pretending to know how to prepare food and drinks. when we finally got to order they had run out of most things we wanted on the limited menu (sausage rolls and sandwiches) so we ended up waiting 20 mins for some chips to be cooked, which was only a small portion. the tables out on the beach were dirty, with rubbish underneath them. a poorly managed place, which clearly trades on its location.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"contracts\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "test[\"token\"] = test[\"contracts\"].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'avoided', 'as', 'we', 'were', 'told', 'the', 'caf√©', 'was', 'full', '(', 'even', 'though', 'there', 'were', 'empty', 'tables', ')', 'we', 'foolishly', 'went', 'to', 'the', 'kiosk', 'outside', 'to', 'order', 'food', 'and', 'drinks', '.', 'it', 'was', 'barely', 'organised', 'chaos', '.', 'the', 'queue', 'just', 'did', 'not', 'move', ',', 'while', 'they', 'faffed', 'about', 'pretending', 'to', 'know', 'how', 'to', 'prepare', 'food', 'and', 'drinks', '.', 'when', 'we', 'finally', 'got', 'to', 'order', 'they', 'had', 'run', 'out', 'of', 'most', 'things', 'we', 'wanted', 'on', 'the', 'limited', 'menu', '(', 'sausage', 'rolls', 'and', 'sandwiches', ')', 'so', 'we', 'ended', 'up', 'waiting', '20', 'mins', 'for', 'some', 'chips', 'to', 'be', 'cooked', ',', 'which', 'was', 'only', 'a', 'small', 'portion', '.', 'the', 'tables', 'out', 'on', 'the', 'beach', 'were', 'dirty', ',', 'with', 'rubbish', 'underneath', 'them', '.', 'a', 'poorly', 'managed', 'place', ',', 'which', 'clearly', 'trades', 'on', 'its', 'location', '.']"
     ]
    }
   ],
   "source": [
    "# Example review\n",
    "print(test[\"token\"].iloc[0], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "punc = string.punctuation\n",
    "test[\"punct\"] = test[\"token\"].apply(lambda x: [word for word in x if word not in punc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'avoided', 'as', 'we', 'were', 'told', 'the', 'caf√©', 'was', 'full', 'even', 'though', 'there', 'were', 'empty', 'tables', 'we', 'foolishly', 'went', 'to', 'the', 'kiosk', 'outside', 'to', 'order', 'food', 'and', 'drinks', 'it', 'was', 'barely', 'organised', 'chaos', 'the', 'queue', 'just', 'did', 'not', 'move', 'while', 'they', 'faffed', 'about', 'pretending', 'to', 'know', 'how', 'to', 'prepare', 'food', 'and', 'drinks', 'when', 'we', 'finally', 'got', 'to', 'order', 'they', 'had', 'run', 'out', 'of', 'most', 'things', 'we', 'wanted', 'on', 'the', 'limited', 'menu', 'sausage', 'rolls', 'and', 'sandwiches', 'so', 'we', 'ended', 'up', 'waiting', '20', 'mins', 'for', 'some', 'chips', 'to', 'be', 'cooked', 'which', 'was', 'only', 'a', 'small', 'portion', 'the', 'tables', 'out', 'on', 'the', 'beach', 'were', 'dirty', 'with', 'rubbish', 'underneath', 'them', 'a', 'poorly', 'managed', 'place', 'which', 'clearly', 'trades', 'on', 'its', 'location']\n"
     ]
    }
   ],
   "source": [
    "# Example review\n",
    "print(test[\"punct\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers, except words that contain numbers.\n",
    "test[\"ex_num\"] = test[\"punct\"].apply(lambda x: [n for n in x if not n.isnumeric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non ascii characters\n",
    "test[\"ascii\"]= test[\"ex_num\"].apply(lambda x: [e for e in x if e.encode(\"ascii\",\"ignore\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'avoided', 'as', 'we', 'were', 'told', 'the', 'caf√©', 'was', 'full', 'even', 'though', 'there', 'were', 'empty', 'tables', 'we', 'foolishly', 'went', 'to', 'the', 'kiosk', 'outside', 'to', 'order', 'food', 'and', 'drinks', 'it', 'was', 'barely', 'organised', 'chaos', 'the', 'queue', 'just', 'did', 'not', 'move', 'while', 'they', 'faffed', 'about', 'pretending', 'to', 'know', 'how', 'to', 'prepare', 'food', 'and', 'drinks', 'when', 'we', 'finally', 'got', 'to', 'order', 'they', 'had', 'run', 'out', 'of', 'most', 'things', 'we', 'wanted', 'on', 'the', 'limited', 'menu', 'sausage', 'rolls', 'and', 'sandwiches', 'so', 'we', 'ended', 'up', 'waiting', 'mins', 'for', 'some', 'chips', 'to', 'be', 'cooked', 'which', 'was', 'only', 'a', 'small', 'portion', 'the', 'tables', 'out', 'on', 'the', 'beach', 'were', 'dirty', 'with', 'rubbish', 'underneath', 'them', 'a', 'poorly', 'managed', 'place', 'which', 'clearly', 'trades', 'on', 'its', 'location']\n"
     ]
    }
   ],
   "source": [
    "# Example review\n",
    "print(test[\"ascii\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stopwords list\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove and add words to stopwords list - expanding contractions eliminated most negation words like 'didn't already \n",
    "# but the word'not' is taken out as contractions convert to 'did not' etc.\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop_remove = [\"not\"]\n",
    "stop_left = [s for s in stop if s not in stop_remove]\n",
    "newStopWords = ['etc']\n",
    "stop_left.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove common/stopwords\n",
    "test[\"no_stop\"] = test[\"ascii\"].apply(lambda x: [w for w in x if w not in stop_left])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'avoided', 'told', 'caf√©', 'full', 'even', 'though', 'empty', 'tables', 'foolishly', 'went', 'kiosk', 'outside', 'order', 'food', 'drinks', 'barely', 'organised', 'chaos', 'queue', 'not', 'move', 'faffed', 'pretending', 'know', 'prepare', 'food', 'drinks', 'finally', 'got', 'order', 'run', 'things', 'wanted', 'limited', 'menu', 'sausage', 'rolls', 'sandwiches', 'ended', 'waiting', 'mins', 'chips', 'cooked', 'small', 'portion', 'tables', 'beach', 'dirty', 'rubbish', 'underneath', 'poorly', 'managed', 'place', 'clearly', 'trades', 'location']\n"
     ]
    }
   ],
   "source": [
    "# Example review\n",
    "print(test[\"no_stop\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(text_chunk):\n",
    "    spell = SpellChecker()\n",
    "    new_list = []\n",
    "    corrected = []\n",
    "    for word in text_chunk:\n",
    "        if spell.correction(word) != word:\n",
    "            new_word = spell.correction(word)\n",
    "            new_list.append(new_word)\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', 'is', 'the', 'best', 'restaurant']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run with example text to show corrections\n",
    "text_example_spelling = [\"where\", \"is\", \"the\", \"best\", \"restarant\"]\n",
    "spell_check(text_example_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1352    [best, avoided, told, cafe, full, even, though...\n",
       "482     [first, class, food, tasteful, surroundings, l...\n",
       "1309    [lunchtime, chaos, booked, table, several, wee...\n",
       "270     [excessive, cost, premier, inn, charged, charg...\n",
       "278     [charming, hotel, booked, stay, dinner, spa, t...\n",
       "665     [secret, gem, beautiful, treat, spa, day, bail...\n",
       "1012    [family, run, restaurant, amazing, food, amazi...\n",
       "1166    [disappointing, start, called, fresh, salad, b...\n",
       "1099    [super, brunch, lovely, surroundings, popped, ...\n",
       "1062    [probably, unfriendlies, place, ever, visited,...\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply to test dataframe\n",
    "test[\"no_stop\"].apply(lambda x: spell_check(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'avoided', 'told', 'caf√©', 'full', 'even', 'though', 'empty', 'tables', 'foolishly', 'went', 'kiosk', 'outside', 'order', 'food', 'drinks', 'barely', 'organised', 'chaos', 'queue', 'not', 'move', 'faffed', 'pretending', 'know', 'prepare', 'food', 'drinks', 'finally', 'got', 'order', 'run', 'things', 'wanted', 'limited', 'menu', 'sausage', 'rolls', 'sandwiches', 'ended', 'waiting', 'mins', 'chips', 'cooked', 'small', 'portion', 'tables', 'beach', 'dirty', 'rubbish', 'underneath', 'poorly', 'managed', 'place', 'clearly', 'trades', 'location']\n"
     ]
    }
   ],
   "source": [
    "print(test[\"no_stop\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize to common root\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "test['lemma'] = test.no_stop.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrgInd</th>\n",
       "      <th>Town</th>\n",
       "      <th>Category</th>\n",
       "      <th>Score</th>\n",
       "      <th>all_text</th>\n",
       "      <th>lower</th>\n",
       "      <th>clean</th>\n",
       "      <th>contracts</th>\n",
       "      <th>token</th>\n",
       "      <th>punct</th>\n",
       "      <th>ex_num</th>\n",
       "      <th>ascii</th>\n",
       "      <th>no_stop</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>564</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Food</td>\n",
       "      <td>1</td>\n",
       "      <td>Best avoided As we were told the caf√© was full...</td>\n",
       "      <td>best avoided as we were told the caf√© was full...</td>\n",
       "      <td>best avoided as we were told the caf√© was full...</td>\n",
       "      <td>best avoided as we were told the caf√© was full...</td>\n",
       "      <td>[best, avoided, as, we, were, told, the, caf√©,...</td>\n",
       "      <td>[best, avoided, as, we, were, told, the, caf√©,...</td>\n",
       "      <td>[best, avoided, as, we, were, told, the, caf√©,...</td>\n",
       "      <td>[best, avoided, as, we, were, told, the, caf√©,...</td>\n",
       "      <td>[best, avoided, told, caf√©, full, even, though...</td>\n",
       "      <td>[best, avoided, told, caf√©, full, even, though...</td>\n",
       "      <td>[(best, RB), (avoided, VBN), (told, NN), (caf√©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>245</td>\n",
       "      <td>Bognor</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "      <td>First class food in tasteful surroundings I ha...</td>\n",
       "      <td>first class food in tasteful surroundings i ha...</td>\n",
       "      <td>first class food in tasteful surroundings i ha...</td>\n",
       "      <td>first class food in tasteful surroundings i ha...</td>\n",
       "      <td>[first, class, food, in, tasteful, surrounding...</td>\n",
       "      <td>[first, class, food, in, tasteful, surrounding...</td>\n",
       "      <td>[first, class, food, in, tasteful, surrounding...</td>\n",
       "      <td>[first, class, food, in, tasteful, surrounding...</td>\n",
       "      <td>[first, class, food, tasteful, surroundings, l...</td>\n",
       "      <td>[first, class, food, tasteful, surroundings, l...</td>\n",
       "      <td>[(first, RB), (class, NN), (food, NN), (tastef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      OrgInd           Town Category  Score  \\\n",
       "1352     564  Littlehampton     Food      1   \n",
       "482      245         Bognor     Food      0   \n",
       "\n",
       "                                               all_text  \\\n",
       "1352  Best avoided As we were told the caf√© was full...   \n",
       "482   First class food in tasteful surroundings I ha...   \n",
       "\n",
       "                                                  lower  \\\n",
       "1352  best avoided as we were told the caf√© was full...   \n",
       "482   first class food in tasteful surroundings i ha...   \n",
       "\n",
       "                                                  clean  \\\n",
       "1352  best avoided as we were told the caf√© was full...   \n",
       "482   first class food in tasteful surroundings i ha...   \n",
       "\n",
       "                                              contracts  \\\n",
       "1352  best avoided as we were told the caf√© was full...   \n",
       "482   first class food in tasteful surroundings i ha...   \n",
       "\n",
       "                                                  token  \\\n",
       "1352  [best, avoided, as, we, were, told, the, caf√©,...   \n",
       "482   [first, class, food, in, tasteful, surrounding...   \n",
       "\n",
       "                                                  punct  \\\n",
       "1352  [best, avoided, as, we, were, told, the, caf√©,...   \n",
       "482   [first, class, food, in, tasteful, surrounding...   \n",
       "\n",
       "                                                 ex_num  \\\n",
       "1352  [best, avoided, as, we, were, told, the, caf√©,...   \n",
       "482   [first, class, food, in, tasteful, surrounding...   \n",
       "\n",
       "                                                  ascii  \\\n",
       "1352  [best, avoided, as, we, were, told, the, caf√©,...   \n",
       "482   [first, class, food, in, tasteful, surrounding...   \n",
       "\n",
       "                                                no_stop  \\\n",
       "1352  [best, avoided, told, caf√©, full, even, though...   \n",
       "482   [first, class, food, tasteful, surroundings, l...   \n",
       "\n",
       "                                                  lemma  \\\n",
       "1352  [best, avoided, told, caf√©, full, even, though...   \n",
       "482   [first, class, food, tasteful, surroundings, l...   \n",
       "\n",
       "                                               pos_tags  \n",
       "1352  [(best, RB), (avoided, VBN), (told, NN), (caf√©...  \n",
       "482   [(first, RB), (class, NN), (food, NN), (tastef...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get parts of speech using NLTK\n",
    "test['pos_tags'] = test['no_stop'].apply(nltk.tag.pos_tag)\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('best', 'RB'), ('avoided', 'VBN'), ('told', 'NN'), ('caf√©', 'NN'), ('full', 'JJ'), ('even', 'RB'), ('though', 'IN'), ('empty', 'JJ'), ('tables', 'NNS'), ('foolishly', 'RB'), ('went', 'VBD'), ('kiosk', 'NNS'), ('outside', 'IN'), ('order', 'NN'), ('food', 'NN'), ('drinks', 'NNS'), ('barely', 'RB'), ('organised', 'VBD'), ('chaos', 'NN'), ('queue', 'NN'), ('not', 'RB'), ('move', 'VB'), ('faffed', 'RB'), ('pretending', 'VBG'), ('know', 'PRP'), ('prepare', 'JJ'), ('food', 'NN'), ('drinks', 'NNS'), ('finally', 'RB'), ('got', 'VBD'), ('order', 'NN'), ('run', 'VB'), ('things', 'NNS'), ('wanted', 'VBD'), ('limited', 'JJ'), ('menu', 'NN'), ('sausage', 'NN'), ('rolls', 'NNS'), ('sandwiches', 'NNS'), ('ended', 'VBD'), ('waiting', 'VBG'), ('mins', 'NNS'), ('chips', 'NNS'), ('cooked', 'VBD'), ('small', 'JJ'), ('portion', 'NN'), ('tables', 'NNS'), ('beach', 'VBP'), ('dirty', 'JJ'), ('rubbish', 'JJ'), ('underneath', 'NN'), ('poorly', 'RB'), ('managed', 'VBD'), ('place', 'NN'), ('clearly', 'RB'), ('trades', 'VBZ'), ('location', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Example item\n",
    "print(test.pos_tags.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best avoided As we were told the caf√© was full (even though there were empty tables) we foolishly went to the kiosk outside to order food and drinks. It was barely organised chaos. The queue just didn't move, while they faffed about pretending to know how to prepare food and drinks. When we finally got to order they had run out of most things we wanted on the limited menu (sausage rolls and sandwiches) so we ended up waiting 20 mins for some chips to be cooked, which was only a small portion. The tables out on the beach were dirty, with rubbish underneath them. A poorly managed place, which clearly trades on its location. \n",
      "\n",
      "['best', 'avoided', 'told', 'caf√©', 'full', 'even', 'though', 'empty', 'table', 'foolishly', 'went', 'kiosk', 'outside', 'order', 'food', 'drink', 'barely', 'organised', 'chaos', 'queue', 'not', 'move', 'faffed', 'pretending', 'know', 'prepare', 'food', 'drink', 'finally', 'got', 'order', 'run', 'thing', 'wanted', 'limited', 'menu', 'sausage', 'roll', 'sandwich', 'ended', 'waiting', 'min', 'chip', 'cooked', 'small', 'portion', 'table', 'beach', 'dirty', 'rubbish', 'underneath', 'poorly', 'managed', 'place', 'clearly', 'trade', 'location'] \n",
      "\n",
      "[('best', 'RB'), ('avoided', 'VBN'), ('told', 'NN'), ('caf√©', 'NN'), ('full', 'JJ'), ('even', 'RB'), ('though', 'IN'), ('empty', 'JJ'), ('tables', 'NNS'), ('foolishly', 'RB'), ('went', 'VBD'), ('kiosk', 'NNS'), ('outside', 'IN'), ('order', 'NN'), ('food', 'NN'), ('drinks', 'NNS'), ('barely', 'RB'), ('organised', 'VBD'), ('chaos', 'NN'), ('queue', 'NN'), ('not', 'RB'), ('move', 'VB'), ('faffed', 'RB'), ('pretending', 'VBG'), ('know', 'PRP'), ('prepare', 'JJ'), ('food', 'NN'), ('drinks', 'NNS'), ('finally', 'RB'), ('got', 'VBD'), ('order', 'NN'), ('run', 'VB'), ('things', 'NNS'), ('wanted', 'VBD'), ('limited', 'JJ'), ('menu', 'NN'), ('sausage', 'NN'), ('rolls', 'NNS'), ('sandwiches', 'NNS'), ('ended', 'VBD'), ('waiting', 'VBG'), ('mins', 'NNS'), ('chips', 'NNS'), ('cooked', 'VBD'), ('small', 'JJ'), ('portion', 'NN'), ('tables', 'NNS'), ('beach', 'VBP'), ('dirty', 'JJ'), ('rubbish', 'JJ'), ('underneath', 'NN'), ('poorly', 'RB'), ('managed', 'VBD'), ('place', 'NN'), ('clearly', 'RB'), ('trades', 'VBZ'), ('location', 'NN')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test.all_text.iloc[0],\"\\n\")\n",
    "print(test.lemma.iloc[0],\"\\n\")\n",
    "print(test.pos_tags.iloc[0],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Text Cleaning Sentence for Demo in Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Nice spacious room, clean and cmfortable beds, we stayed 3 nights and I couldn't fault anything! üòä read less\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nice spacious room, clean and cmfortable beds, we stayed 3 nights and i couldn't fault anything! üòä read less\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = example.strip().lower()\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nice spacious room, clean and cmfortable beds, we stayed 3 nights and i couldn't fault anything! üòä \""
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = example.replace('read less', '')\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nice spacious room, clean and cmfortable beds, we stayed 3 nights and i could not fault anything! üòä '"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = contractions.fix(example)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'spacious', 'room', ',', 'clean', 'and', 'cmfortable', 'beds', ',', 'we', 'stayed', '3', 'nights', 'and', 'i', 'could', 'not', 'fault', 'anything', '!', 'üòä']\n"
     ]
    }
   ],
   "source": [
    "example = nltk.word_tokenize(example)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'spacious', 'room', 'clean', 'and', 'cmfortable', 'beds', 'we', 'stayed', '3', 'nights', 'and', 'i', 'could', 'not', 'fault', 'anything', 'üòä']\n"
     ]
    }
   ],
   "source": [
    "example = [word for word in example if word not in punc]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'spacious', 'room', 'clean', 'and', 'cmfortable', 'beds', 'we', 'stayed', '3', 'nights', 'and', 'i', 'could', 'not', 'fault', 'anything']\n"
     ]
    }
   ],
   "source": [
    "example = [e for e in example if e.encode(\"ascii\",\"ignore\")]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'spacious', 'room', 'clean', 'and', 'cmfortable', 'beds', 'we', 'stayed', 'nights', 'and', 'i', 'could', 'not', 'fault', 'anything']\n"
     ]
    }
   ],
   "source": [
    "example = [n for n in example if not n.isnumeric()]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'spacious', 'room', 'clean', 'cmfortable', 'beds', 'stayed', 'nights', 'could', 'not', 'fault', 'anything']\n"
     ]
    }
   ],
   "source": [
    "example = [w for w in example if w not in stop_left]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(text_chunk):\n",
    "    spell = SpellChecker()\n",
    "    new_list = []\n",
    "    corrected = []\n",
    "    for word in text_chunk:\n",
    "        if spell.correction(word) != word:\n",
    "            new_word = spell.correction(word)\n",
    "            new_list.append(new_word)\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'spacious', 'room', 'clean', 'comfortable', 'beds', 'stayed', 'nights', 'could', 'not', 'fault', 'anything']\n"
     ]
    }
   ],
   "source": [
    "example2 = spell_check(example)\n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'spacious', 'room', 'clean', 'comfortable', 'bed', 'stayed', 'night', 'could', 'not', 'fault', 'anything']\n"
     ]
    }
   ],
   "source": [
    "example = lemmatize_text(example2)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nice', 'RB'), ('spacious', 'JJ'), ('room', 'NN'), ('clean', 'NN'), ('comfortable', 'JJ'), ('bed', 'NN'), ('stayed', 'VBD'), ('night', 'NN'), ('could', 'MD'), ('not', 'RB'), ('fault', 'VB'), ('anything', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tag.pos_tag(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Text cleaning function applied to the combined dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a pre-processing function based on the steps outlined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General pre-processing function with the above tasks to clean text -\n",
    "# spellchecker can replace words with incorrect words that do not make sense so can be uncommented to run if necessary\n",
    "\n",
    "def process(text):\n",
    "    \n",
    "       \n",
    "    # Strip whitespace and set to lowercase\n",
    "    text = text.apply(lambda x: x.strip().lower())\n",
    "    \n",
    "    # Replace words, and remove the 'read more', 'read less' tags \n",
    "    text = text.apply(lambda x: x.replace('xmas','christmas'))\n",
    "    text = text.apply(lambda x: x.replace('\\nread less',\"\"))\n",
    "    text = text.apply(lambda x: x.replace('\\nread more',\"\"))\n",
    "       \n",
    "                  \n",
    "    # Clean other issues with text\n",
    "    text = text.replace({'\\¬£':''}, regex = True) # remove pound sign\n",
    "    text = text.replace(r'\\/',\" \", regex=True) # split words separated with slash\n",
    "    text = text.replace({'\\d+\\-\\d+':\"\"}, regex = True) # remove digits\n",
    "    text = text.replace({'\\d+\\w{2}':\"\"}, regex = True) # remove number plus am, pm, th or nd\n",
    "    text = text.replace({'\\.{3,}':\"\"}, regex = True) # remove multiple full stops not removed by punctuation\n",
    "    \n",
    "   # Expand contractions\n",
    "    text = text.apply(lambda x: contractions.fix(x))\n",
    "\n",
    "    # Tokenize text\n",
    "    text = text.apply(lambda x: nltk.word_tokenize(x))\n",
    "    \n",
    "    # Remove punctuation\n",
    "    punc = string.punctuation\n",
    "    text = text.apply(lambda x: [word for word in x if word not in punc])\n",
    "             \n",
    "    # Remove numbers, except words that contain numbers.\n",
    "    text = text.apply(lambda x: [n for n in x if not n.isnumeric()])\n",
    "\n",
    "    # Remove non ascii characters\n",
    "    text = text.apply(lambda x: [e for e in x if e.encode(\"ascii\",\"ignore\")])\n",
    "\n",
    "    # Remove common/stopwords - extend and remove some words from the list as negation words might be important to retain\n",
    "    stop = stopwords.words('english')\n",
    "    stop_remove =[\"not\",\"don't\",\"didn't\",\"wasn't\",\"won't\",\"isn't\"]\n",
    "    stop1 = [elem for elem in stop if elem not in stop_remove] \n",
    "    add_stop = ['etc','read','butlins', 'bognor','regis','b',' i '\n",
    "                '..','arundel castle','premier','inn','u','castle',\n",
    "                \"year\",\"hilton\",\"time\",\"day\",\"shoreline\",\"oyster\",\"bay\",\"church farm\"]\n",
    "    stop1.extend(add_stop)\n",
    "    text = text.apply(lambda x: [w for w in x if w not in stop1])\n",
    "    \n",
    "    # Run spellchecker - (uncomment to run)\n",
    "   # def spell_check(text):\n",
    "        #spell = SpellChecker()\n",
    "        #text2 = []\n",
    "        #corrected = []\n",
    "        #for word in text:\n",
    "           # if spell.correction(word) != word:\n",
    "               # new_word = spell.correction(word)\n",
    "                #text2.append(new_word)\n",
    "           # else:\n",
    "                #text2.append(word)\n",
    "       # return text2\n",
    "    \n",
    "    text = text.apply(lambda x: spell_check(x))\n",
    "             \n",
    "    # Lemmatize to common root\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize_text(text):\n",
    "        return [lemmatizer.lemmatize(w) for w in text]\n",
    "    \n",
    "    text = text.apply(lemmatize_text)\n",
    "    \n",
    "    # Convert list to string \n",
    "    text = text.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Remove trailing 'i'\n",
    "    text = text.apply(lambda x: x.replace(' i ',\"\"))\n",
    "  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function - uncomment to run as resource intensive\n",
    "df_train_cleaned = process(df_train[\"all_text\"])\n",
    "df_val_cleaned = process(df_val[\"all_text\"])\n",
    "df_test_cleaned = process(df_test[\"all_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make dataframe, name column then find parts of speech for text in that column\n",
    "def make_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = [\"text_clean\"]\n",
    "    df['pos'] = df[\"text_clean\"].apply(lambda x:nltk.tag.pos_tag(x.split()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function\n",
    "df_train_cleaned = make_df(df_train_cleaned)\n",
    "df_val_cleaned = make_df(df_val_cleaned)\n",
    "df_test_cleaned = make_df(df_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concat two dataframes and name columns\n",
    "def convert(df,df2):\n",
    "    df = pd.concat([df2,df],axis = 1)\n",
    "    columns = \"OrgInd\",\"Town\",\"Category\",\"Score\",\"Sent\",\"Sent_clean\",\"Pos\"\n",
    "    df.columns = (columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function\n",
    "train_cleaned = convert(df_train_cleaned,df_train)\n",
    "val_cleaned = convert(df_val_cleaned,df_val)\n",
    "test_cleaned = convert(df_test_cleaned,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrgInd</th>\n",
       "      <th>Town</th>\n",
       "      <th>Category</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sent</th>\n",
       "      <th>Sent_clean</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245</td>\n",
       "      <td>Bognor</td>\n",
       "      <td>Accommodation</td>\n",
       "      <td>0</td>\n",
       "      <td>Central B &amp; B We had room 6, excellent view, w...</td>\n",
       "      <td>central room excellent view could see sea room...</td>\n",
       "      <td>[(central, JJ), (room, NN), (excellent, JJ), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209</td>\n",
       "      <td>Littlehampton</td>\n",
       "      <td>Accommodation</td>\n",
       "      <td>1</td>\n",
       "      <td>Not what it used to be... We lived in the area...</td>\n",
       "      <td>not used lived area year fact daughter worked ...</td>\n",
       "      <td>[(not, RB), (used, VBN), (lived, VBN), (area, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrgInd           Town       Category  Score  \\\n",
       "0     245         Bognor  Accommodation      0   \n",
       "1     209  Littlehampton  Accommodation      1   \n",
       "\n",
       "                                                Sent  \\\n",
       "0  Central B & B We had room 6, excellent view, w...   \n",
       "1  Not what it used to be... We lived in the area...   \n",
       "\n",
       "                                          Sent_clean  \\\n",
       "0  central room excellent view could see sea room...   \n",
       "1  not used lived area year fact daughter worked ...   \n",
       "\n",
       "                                                 Pos  \n",
       "0  [(central, JJ), (room, NN), (excellent, JJ), (...  \n",
       "1  [(not, RB), (used, VBN), (lived, VBN), (area, ...  "
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleaned.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send cleaned and combined dataframes to csv\n",
    "train_cleaned.to_csv(\"train_cleaned.csv\")\n",
    "val_cleaned.to_csv(\"val_cleaned.csv\")\n",
    "test_cleaned.to_csv(\"test_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
